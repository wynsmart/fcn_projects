<PERSONAL INFO>
name: Fan Yang
nuid: 00-179-7661
email: yang.fan7@husky.neu.edu

name: Samkeet Shah
nuid: 001258979
email: shah.sam@husky.neu.edu


<HIGH-LEVEL APPROACH>
- ./webcrawler command calls the main.py with the given arguments.
- main.py calls utils.py to initialize the logger and parameter parser for the
  given application
- main.py initializes an Object of MyCrawler passing the root path of the fakebook.com
  MyCrawler creates a MyPaw object to login to the website using the given
  arguments (username and password) using a HTTP POST request.
- Further it creates a HTTP wrapper to send and receive custom HTTP responses using
  HTTP GET requests.
- The crawler starts crawling from the root URL after successful login and
  performs a BFS in multi-threaded mode to scan for <a href="/..."> tags and
  <h2 class = "secret_flags"> tags using a regex.
- the crawler maintains a 'seen' set to store the list of URLs seen before, then
  pops one from the queue and adds the links found on that page to unvisited list.
- It stores all secret_flags found in a secret_flags dictionary and prints all
  flags after crawling all pages.


<CHALLENGES>
1. Choosing the right algorithm BFS vs DFS
Depth first search is slower as compared to Breadth First Search for
getting all unique pages faster according to practical tests. Hence we opted
for BFS over DFS.

2. Speeding up the crawling (threads)
Initial crawling took a lot of time (about 20mins) to crawl all of 10955 pages
to ensure we had found all possible flags apart from the 5 mentioned.
To speed the up the crawling process we firstly reduced the choking time of socket
recv method. The socket recv method may not receive complete data, however, pure
'while' loop until no data sometimes chokes the thread, waiting for socket close.
So finally we added some special cases detection to end receiving early. What's
more, we added multi-threading (defaults to 5 threads) to crawl the website
simultaneously. Eventually, we reduced the execution time to around one minute.

3. Using Regex to Parse HTML pages
Since we weren't allowed to use 3rd party libraries to parse, it posed a problem
parsing webpage HTML source to get the specific URLs and secret_flags using
regular expressions.

4. Content-Length in the HTTP Header
Trouble figuring out initial HTTP responses from the server due to missing field
Content-Length in the HTTP header. The server replied with a 'CSRF not allowed'
message.


<TESTING>
1. Testing without multi-threading. (took > 20 mins to crawl)
2. Testing with multi-threading (takes upto 1.5 mins)
3. Handling errors due to fewer arguments.


<WORK LOAD>
Fan Yang:
- Parsed arguments;
- Handled socket connection and requests;
- Reverse engineered login page to implement Fakebook login;
- Provided 'fetch' api to get data of given path;
- [Files] utils.py, conns.py

Samkeet Shah:
- Implemented basic BFS algorithm with 'fetch' api;
- Improved BFS with multi-threading to speed up;
- Assembled the main entry function
- [Files] main.py, crawler.py
